{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tiny GPT\n",
    "Build a small GPT model using PyTorch (in mac)"
   ],
   "id": "118228ff3fc9d889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check PyTorch instance",
   "id": "7553bb1b386d8737"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_Notes:_\n",
    "\n",
    "**MPS (Metal Performance Shaders)**\n",
    "* Metal Performance Shaders is an Apple framework of highly optimized GPU shaders for image processing, linear algebra, and neural networks on top of Metal.\n",
    "* PyTorch, diffusers, and other ML libraries expose an mps device to offload tensor operations to the GPU via Metal/MPS on Apple silicon.\n"
   ],
   "id": "ca31039f830b58ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.688100Z",
     "start_time": "2025-12-27T11:20:01.647462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Torch version: {torch.__version__} and MPS availability is: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "print(f\"Default device set to: {torch.get_default_device()}\")\n",
    "print(f\"Is MPS built: {torch.backends.mps.is_built()}\")"
   ],
   "id": "394534f77af7eb6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1 and MPS availability is: True\n",
      "Default device set to: mps:0\n",
      "Is MPS built: True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mathematical Explanation\n",
    "\n",
    "A LLM model only understands numbers - based on which it calculates the probability. The model selects the next one word that has got the highest probability.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w_1,w_2,w_3,......,w_n) = \\prod_{t=1}^{n}P(w_t|w_1,w_2,......,w_{t-1}) \\\\\n",
    "\\text{where, } w_i \\text{ represents a token}\n",
    "\\end{aligned}\n",
    "$$\n",
    "E.g., suppose,\n",
    "* we have the vocabulary (training set): [`very`, `tea`, `hot`, `is`, `the`]\n",
    "* the first 2 words fed to the model is: [`the`, `tea`]\n",
    "* the model will use the above formula of _Chain Probability_ to predict the next likely word.\n",
    "    * So, it will execute `P('is'|'the','tea')`. This reads as probability of `is` given the previous words are `the` and `tea`. Similarly it will execute `P('hot'|'the','tea')` and so on. The one with the highest probability wins.\n",
    "    * For a set of 4 tokens, the overall probability is calculated as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w_1,w_2,w_3,w_4) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\times P(w_4|w_1, w_2, w_3)\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "id": "10c93a036a6f2dd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Data",
   "id": "fbe378440a79427b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.710571Z",
     "start_time": "2025-12-27T11:20:01.702569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [\n",
    "    \"hello friends how are you\",\n",
    "    \"the tea is very hot\",\n",
    "    \"my name is Sushovan\",\n",
    "    \"the roads of Kolkata are busy\",\n",
    "    \"it is raining in Mumbai\",\n",
    "    \"the train is late again\",\n",
    "    \"i love eating samosas and drinking tea\",\n",
    "    \"holi is my favorite festival\",\n",
    "    \"diwali brings lights and sweets\",\n",
    "    \"india won the cricket match\"\n",
    "]"
   ],
   "id": "e6facecd2fbfcc27",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenize Data for the model\n",
    "_Note_: Instead of using a tokenizer library (e.g. `BPE` - `Byte Pair Encoding` used in GPT or `SentencePiece` used in LLAMA), we shall be building a custom tokenizer to understand the concepts."
   ],
   "id": "eb7486fd996c3e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Put end marker and concatenate the corpus",
   "id": "2aaafa193bd04b4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.753142Z",
     "start_time": "2025-12-27T11:20:01.712628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \" \".join([s + \" <END>\" for s in corpus])\n",
    "print(text)"
   ],
   "id": "cef642771ea40b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello friends how are you <END> the tea is very hot <END> my name is Sushovan <END> the roads of Kolkata are busy <END> it is raining in Mumbai <END> the train is late again <END> i love eating samosas and drinking tea <END> holi is my favorite festival <END> diwali brings lights and sweets <END> india won the cricket match <END>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract the words to build the vocabulary",
   "id": "8e9b4cfa3335d502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.786883Z",
     "start_time": "2025-12-27T11:20:01.754929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = list(set(text.split()))\n",
    "vocab_size = len(words)\n",
    "print(f\"Vocabulary: {words}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ],
   "id": "aded0c62368a5545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['of', 'lights', 'it', 'love', 'name', 'you', 'are', 'in', 'i', 'Kolkata', 'and', 'diwali', 'match', 'how', 'train', 'tea', 'sweets', '<END>', 'samosas', 'hello', 'very', 'raining', 'the', 'is', 'festival', 'Mumbai', 'drinking', 'holi', 'favorite', 'roads', 'busy', 'hot', 'eating', 'won', 'india', 'late', 'my', 'again', 'friends', 'Sushovan', 'cricket', 'brings']\n",
      "Vocabulary Size: 42\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build the word index",
   "id": "6d3b521eee5a5043"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.806240Z",
     "start_time": "2025-12-27T11:20:01.789720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2idx = {w: idx for idx, w in enumerate(words)}\n",
    "print(f\"Words to Index:  {word2idx}\")\n",
    "\n",
    "idx2word = {idx: w for w, idx in word2idx.items()}\n",
    "print(f\"Index to Words:  {idx2word}\")"
   ],
   "id": "fc89f08bb3def977",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to Index:  {'of': 0, 'lights': 1, 'it': 2, 'love': 3, 'name': 4, 'you': 5, 'are': 6, 'in': 7, 'i': 8, 'Kolkata': 9, 'and': 10, 'diwali': 11, 'match': 12, 'how': 13, 'train': 14, 'tea': 15, 'sweets': 16, '<END>': 17, 'samosas': 18, 'hello': 19, 'very': 20, 'raining': 21, 'the': 22, 'is': 23, 'festival': 24, 'Mumbai': 25, 'drinking': 26, 'holi': 27, 'favorite': 28, 'roads': 29, 'busy': 30, 'hot': 31, 'eating': 32, 'won': 33, 'india': 34, 'late': 35, 'my': 36, 'again': 37, 'friends': 38, 'Sushovan': 39, 'cricket': 40, 'brings': 41}\n",
      "Index to Words:  {0: 'of', 1: 'lights', 2: 'it', 3: 'love', 4: 'name', 5: 'you', 6: 'are', 7: 'in', 8: 'i', 9: 'Kolkata', 10: 'and', 11: 'diwali', 12: 'match', 13: 'how', 14: 'train', 15: 'tea', 16: 'sweets', 17: '<END>', 18: 'samosas', 19: 'hello', 20: 'very', 21: 'raining', 22: 'the', 23: 'is', 24: 'festival', 25: 'Mumbai', 26: 'drinking', 27: 'holi', 28: 'favorite', 29: 'roads', 30: 'busy', 31: 'hot', 32: 'eating', 33: 'won', 34: 'india', 35: 'late', 36: 'my', 37: 'again', 38: 'friends', 39: 'Sushovan', 40: 'cricket', 41: 'brings'}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convert to tensor\n",
    "Replace each word in text with the corresponding index and fed that to a tensor"
   ],
   "id": "c1a24bd5e8ab22fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.835105Z",
     "start_time": "2025-12-27T11:20:01.806979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor([word2idx[idx] for idx in text.split()], dtype=torch.long)\n",
    "print(f\"Tensor Data: {data}\")\n",
    "print(f\"Data Shape: {data.shape}\")"
   ],
   "id": "8ce9d73bf21ca37d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Data: tensor([19, 38, 13,  6,  5, 17, 22, 15, 23, 20, 31, 17, 36,  4, 23, 39, 17, 22,\n",
      "        29,  0,  9,  6, 30, 17,  2, 23, 21,  7, 25, 17, 22, 14, 23, 35, 37, 17,\n",
      "         8,  3, 32, 18, 10, 26, 15, 17, 27, 23, 36, 28, 24, 17, 11, 41,  1, 10,\n",
      "        16, 17, 34, 33, 22, 40, 12, 17], device='mps:0')\n",
      "Data Shape: torch.Size([62])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Parameters",
   "id": "49766130e430a24e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`block_size`: also known as context_length. It means that how many previous words the llm would refer, to predict the next word",
   "id": "2028cc337be30be2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.840417Z",
     "start_time": "2025-12-27T11:20:01.836084Z"
    }
   },
   "cell_type": "code",
   "source": "block_size = 6",
   "id": "e74b16de9dc1fb86",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`embedding_dim`: Embedding Dimension\n",
    "\n",
    "Each word in the tensor will be represented in the llm model as a dimensional vector of a defined size. Initially, random numbers are generate for each value in the dimensional vector. The llm uses this vector to predict the next word."
   ],
   "id": "2e859c493861ac7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.844640Z",
     "start_time": "2025-12-27T11:20:01.840933Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_dim = 32",
   "id": "ff3d173e49ccd307",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.848394Z",
     "start_time": "2025-12-27T11:20:01.845081Z"
    }
   },
   "cell_type": "code",
   "source": "n_heads = 2  # number of Multi-head attention layer",
   "id": "cd196a5527526411",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.852497Z",
     "start_time": "2025-12-27T11:20:01.848860Z"
    }
   },
   "cell_type": "code",
   "source": "n_layers = 2  # number of transformer blocks to use",
   "id": "bba67c5b4611a456",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.856545Z",
     "start_time": "2025-12-27T11:20:01.852970Z"
    }
   },
   "cell_type": "code",
   "source": "lr = 1e-3  # learning rate",
   "id": "131917c8e2d869eb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:01.861229Z",
     "start_time": "2025-12-27T11:20:01.856957Z"
    }
   },
   "cell_type": "code",
   "source": "epochs = 1500  # number of training iterations",
   "id": "5726df92e353910b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define batch function",
   "id": "b49108b7ca8dc59c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`batch_size`: This indicates the number of sequences (or sentences) for the model to consider",
   "id": "2dd015dabd7b4066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:02.021083Z",
     "start_time": "2025-12-27T11:20:01.861977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(batch_size=16):\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    # the above function gives 16 examples of random sentences\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return ix, x, y\n",
    "\n",
    "\n",
    "rand_int, input_batch, output_bath = get_batch()\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Batch Index: {rand_int}\")\n",
    "print(f\"input_batch shape: {input_batch.data}\")\n",
    "print(f\"output_bath shape: {output_bath.data}\")"
   ],
   "id": "684d39b37702b8ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: tensor([19, 38, 13,  6,  5, 17, 22, 15, 23, 20, 31, 17, 36,  4, 23, 39, 17, 22,\n",
      "        29,  0,  9,  6, 30, 17,  2, 23, 21,  7, 25, 17, 22, 14, 23, 35, 37, 17,\n",
      "         8,  3, 32, 18, 10, 26, 15, 17, 27, 23, 36, 28, 24, 17, 11, 41,  1, 10,\n",
      "        16, 17, 34, 33, 22, 40, 12, 17], device='mps:0')\n",
      "Batch Index: tensor([52, 20, 22,  0, 53, 39, 41, 27, 50, 31, 34, 34, 25, 19, 29,  8],\n",
      "       device='mps:0')\n",
      "input_batch shape: tensor([[ 1, 10, 16, 17, 34, 33],\n",
      "        [ 9,  6, 30, 17,  2, 23],\n",
      "        [30, 17,  2, 23, 21,  7],\n",
      "        [19, 38, 13,  6,  5, 17],\n",
      "        [10, 16, 17, 34, 33, 22],\n",
      "        [18, 10, 26, 15, 17, 27],\n",
      "        [26, 15, 17, 27, 23, 36],\n",
      "        [ 7, 25, 17, 22, 14, 23],\n",
      "        [11, 41,  1, 10, 16, 17],\n",
      "        [14, 23, 35, 37, 17,  8],\n",
      "        [37, 17,  8,  3, 32, 18],\n",
      "        [37, 17,  8,  3, 32, 18],\n",
      "        [23, 21,  7, 25, 17, 22],\n",
      "        [ 0,  9,  6, 30, 17,  2],\n",
      "        [17, 22, 14, 23, 35, 37],\n",
      "        [23, 20, 31, 17, 36,  4]], device='mps:0')\n",
      "output_bath shape: tensor([[10, 16, 17, 34, 33, 22],\n",
      "        [ 6, 30, 17,  2, 23, 21],\n",
      "        [17,  2, 23, 21,  7, 25],\n",
      "        [38, 13,  6,  5, 17, 22],\n",
      "        [16, 17, 34, 33, 22, 40],\n",
      "        [10, 26, 15, 17, 27, 23],\n",
      "        [15, 17, 27, 23, 36, 28],\n",
      "        [25, 17, 22, 14, 23, 35],\n",
      "        [41,  1, 10, 16, 17, 34],\n",
      "        [23, 35, 37, 17,  8,  3],\n",
      "        [17,  8,  3, 32, 18, 10],\n",
      "        [17,  8,  3, 32, 18, 10],\n",
      "        [21,  7, 25, 17, 22, 14],\n",
      "        [ 9,  6, 30, 17,  2, 23],\n",
      "        [22, 14, 23, 35, 37, 17],\n",
      "        [20, 31, 17, 36,  4, 23]], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:02.087322Z",
     "start_time": "2025-12-27T11:20:02.021720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Decoded data: \\n{[idx2word[i.item()] for i in data]}\")\n",
    "print(f\"input stack words: \\n{np.array([[idx2word[i.item()] for i in b] for b in input_batch])}\")\n",
    "print(f\"output stack words: \\n{np.array([[idx2word[i.item()] for i in b] for b in output_bath])}\")"
   ],
   "id": "16c203bbe77abd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded data: \n",
      "['hello', 'friends', 'how', 'are', 'you', '<END>', 'the', 'tea', 'is', 'very', 'hot', '<END>', 'my', 'name', 'is', 'Sushovan', '<END>', 'the', 'roads', 'of', 'Kolkata', 'are', 'busy', '<END>', 'it', 'is', 'raining', 'in', 'Mumbai', '<END>', 'the', 'train', 'is', 'late', 'again', '<END>', 'i', 'love', 'eating', 'samosas', 'and', 'drinking', 'tea', '<END>', 'holi', 'is', 'my', 'favorite', 'festival', '<END>', 'diwali', 'brings', 'lights', 'and', 'sweets', '<END>', 'india', 'won', 'the', 'cricket', 'match', '<END>']\n",
      "input stack words: \n",
      "[['lights' 'and' 'sweets' '<END>' 'india' 'won']\n",
      " ['Kolkata' 'are' 'busy' '<END>' 'it' 'is']\n",
      " ['busy' '<END>' 'it' 'is' 'raining' 'in']\n",
      " ['hello' 'friends' 'how' 'are' 'you' '<END>']\n",
      " ['and' 'sweets' '<END>' 'india' 'won' 'the']\n",
      " ['samosas' 'and' 'drinking' 'tea' '<END>' 'holi']\n",
      " ['drinking' 'tea' '<END>' 'holi' 'is' 'my']\n",
      " ['in' 'Mumbai' '<END>' 'the' 'train' 'is']\n",
      " ['diwali' 'brings' 'lights' 'and' 'sweets' '<END>']\n",
      " ['train' 'is' 'late' 'again' '<END>' 'i']\n",
      " ['again' '<END>' 'i' 'love' 'eating' 'samosas']\n",
      " ['again' '<END>' 'i' 'love' 'eating' 'samosas']\n",
      " ['is' 'raining' 'in' 'Mumbai' '<END>' 'the']\n",
      " ['of' 'Kolkata' 'are' 'busy' '<END>' 'it']\n",
      " ['<END>' 'the' 'train' 'is' 'late' 'again']\n",
      " ['is' 'very' 'hot' '<END>' 'my' 'name']]\n",
      "output stack words: \n",
      "[['and' 'sweets' '<END>' 'india' 'won' 'the']\n",
      " ['are' 'busy' '<END>' 'it' 'is' 'raining']\n",
      " ['<END>' 'it' 'is' 'raining' 'in' 'Mumbai']\n",
      " ['friends' 'how' 'are' 'you' '<END>' 'the']\n",
      " ['sweets' '<END>' 'india' 'won' 'the' 'cricket']\n",
      " ['and' 'drinking' 'tea' '<END>' 'holi' 'is']\n",
      " ['tea' '<END>' 'holi' 'is' 'my' 'favorite']\n",
      " ['Mumbai' '<END>' 'the' 'train' 'is' 'late']\n",
      " ['brings' 'lights' 'and' 'sweets' '<END>' 'india']\n",
      " ['is' 'late' 'again' '<END>' 'i' 'love']\n",
      " ['<END>' 'i' 'love' 'eating' 'samosas' 'and']\n",
      " ['<END>' 'i' 'love' 'eating' 'samosas' 'and']\n",
      " ['raining' 'in' 'Mumbai' '<END>' 'the' 'train']\n",
      " ['Kolkata' 'are' 'busy' '<END>' 'it' 'is']\n",
      " ['the' 'train' 'is' 'late' 'again' '<END>']\n",
      " ['very' 'hot' '<END>' 'my' 'name' 'is']]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build the model",
   "id": "f91ca4a3fca56216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:02.103483Z",
     "start_time": "2025-12-27T11:20:02.088575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformer.transformer_utils import Block\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(block_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_embeddings(idx)\n",
    "        position_embeddings = self.positional_embeddings(torch.arange(T, device=idx.device))\n",
    "\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens=10):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, 1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n"
   ],
   "id": "297fd8705f35d6d7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "7229c54e89b5e2e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:20:52.759873Z",
     "start_time": "2025-12-27T11:20:02.105635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TinyGPT()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for step in range(epochs):\n",
    "    _, xb, yb = get_batch()\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {loss.item():.4f}\")\n",
    "    elif step == epochs - 1:\n",
    "        print(f\"Step: {step}, Loss: {loss.item():.4f}\")"
   ],
   "id": "bdc36c7ac8afac31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 4.0531\n",
      "Step: 100, Loss: 1.2058\n",
      "Step: 200, Loss: 0.3526\n",
      "Step: 300, Loss: 0.2506\n",
      "Step: 400, Loss: 0.1945\n",
      "Step: 500, Loss: 0.1063\n",
      "Step: 600, Loss: 0.1407\n",
      "Step: 700, Loss: 0.2601\n",
      "Step: 800, Loss: 0.0954\n",
      "Step: 900, Loss: 0.1371\n",
      "Step: 1000, Loss: 0.0775\n",
      "Step: 1100, Loss: 0.1871\n",
      "Step: 1200, Loss: 0.0989\n",
      "Step: 1300, Loss: 0.1579\n",
      "Step: 1400, Loss: 0.0543\n",
      "Step: 1499, Loss: 0.1471\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Execute the model",
   "id": "8fbe7240922088ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:26:19.554584Z",
     "start_time": "2025-12-27T11:26:19.540459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_model(context):\n",
    "    word_indexed = [word2idx[word] for word in context.split()]\n",
    "    context = torch.tensor([word_indexed], dtype=torch.long)\n",
    "    out = model.generate(context)\n",
    "\n",
    "    print(\"\\nGenerated Text:\\n\")\n",
    "    print(\" \".join(idx2word[int(i)] for i in out[0]))"
   ],
   "id": "f66524b8ee3dc19e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:27:20.011390Z",
     "start_time": "2025-12-27T11:27:19.929521Z"
    }
   },
   "cell_type": "code",
   "source": "execute_model(\"hello\")",
   "id": "62a59adeaff97bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "hello friends how are you <END> the tea is very hot\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:27:23.041243Z",
     "start_time": "2025-12-27T11:27:22.965094Z"
    }
   },
   "cell_type": "code",
   "source": "execute_model(\"my name\")",
   "id": "8967a2cbb48aacdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "my name is Sushovan <END> the roads of Kolkata are busy <END>\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
