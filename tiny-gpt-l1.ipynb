{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tiny GPT\n",
    "Build a small GPT model using PyTorch (in mac)"
   ],
   "id": "118228ff3fc9d889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check PyTorch instance",
   "id": "7553bb1b386d8737"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_Notes:_\n",
    "\n",
    "**MPS (Metal Performance Shaders)**\n",
    "* Metal Performance Shaders is an Apple framework of highly optimized GPU shaders for image processing, linear algebra, and neural networks on top of Metal.\n",
    "* PyTorch, diffusers, and other ML libraries expose an mps device to offload tensor operations to the GPU via Metal/MPS on Apple silicon.\n"
   ],
   "id": "ca31039f830b58ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.775323Z",
     "start_time": "2025-12-27T11:34:11.889841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Torch version: {torch.__version__} and MPS availability is: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "print(f\"Default device set to: {torch.get_default_device()}\")\n",
    "print(f\"Is MPS built: {torch.backends.mps.is_built()}\")"
   ],
   "id": "394534f77af7eb6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1 and MPS availability is: True\n",
      "Default device set to: mps:0\n",
      "Is MPS built: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mathematical Explanation\n",
    "\n",
    "A LLM model only understands numbers - based on which it calculates the probability. The model selects the next one word that has got the highest probability.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w_1,w_2,w_3,......,w_n) = \\prod_{t=1}^{n}P(w_t|w_1,w_2,......,w_{t-1}) \\\\\n",
    "\\text{where, } w_i \\text{ represents a token}\n",
    "\\end{aligned}\n",
    "$$\n",
    "E.g., suppose,\n",
    "* we have the vocabulary (training set): [`very`, `tea`, `hot`, `is`, `the`]\n",
    "* the first 2 words fed to the model is: [`the`, `tea`]\n",
    "* the model will use the above formula of _Chain Probability_ to predict the next likely word.\n",
    "    * So, it will execute `P('is'|'the','tea')`. This reads as probability of `is` given the previous words are `the` and `tea`. Similarly it will execute `P('hot'|'the','tea')` and so on. The one with the highest probability wins.\n",
    "    * For a set of 4 tokens, the overall probability is calculated as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w_1,w_2,w_3,w_4) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_1, w_2) \\times P(w_4|w_1, w_2, w_3)\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "id": "10c93a036a6f2dd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Data",
   "id": "fbe378440a79427b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.782675Z",
     "start_time": "2025-12-27T11:34:14.778353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [\n",
    "    \"hello friends how are you\",\n",
    "    \"the tea is very hot\",\n",
    "    \"my name is Sushovan\",\n",
    "    \"the roads of Kolkata are busy\",\n",
    "    \"it is raining in Mumbai\",\n",
    "    \"the train is late again\",\n",
    "    \"i love eating samosas and drinking tea\",\n",
    "    \"holi is my favorite festival\",\n",
    "    \"diwali brings lights and sweets\",\n",
    "    \"india won the cricket match\"\n",
    "]"
   ],
   "id": "e6facecd2fbfcc27",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenize Data for the model\n",
    "_Note_: Instead of using a tokenizer library (e.g. `BPE` - `Byte Pair Encoding` used in GPT or `SentencePiece` used in LLAMA), we shall be building a custom tokenizer to understand the concepts."
   ],
   "id": "eb7486fd996c3e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Put end marker and concatenate the corpus",
   "id": "2aaafa193bd04b4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.796225Z",
     "start_time": "2025-12-27T11:34:14.783192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \" \".join([s + \" <END>\" for s in corpus])\n",
    "print(text)"
   ],
   "id": "cef642771ea40b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello friends how are you <END> the tea is very hot <END> my name is Sushovan <END> the roads of Kolkata are busy <END> it is raining in Mumbai <END> the train is late again <END> i love eating samosas and drinking tea <END> holi is my favorite festival <END> diwali brings lights and sweets <END> india won the cricket match <END>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extract the words to build the vocabulary",
   "id": "8e9b4cfa3335d502"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.811977Z",
     "start_time": "2025-12-27T11:34:14.798468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = list(set(text.split()))\n",
    "vocab_size = len(words)\n",
    "print(f\"Vocabulary: {words}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ],
   "id": "aded0c62368a5545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['samosas', 'i', 'lights', 'friends', '<END>', 'my', 'you', 'Sushovan', 'of', 'train', 'and', 'drinking', 'eating', 'holi', 'love', 'sweets', 'very', 'roads', 'busy', 'it', 'again', 'name', 'raining', 'festival', 'brings', 'hot', 'late', 'hello', 'tea', 'the', 'are', 'cricket', 'Mumbai', 'diwali', 'won', 'match', 'is', 'in', 'favorite', 'india', 'Kolkata', 'how']\n",
      "Vocabulary Size: 42\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build the word index",
   "id": "6d3b521eee5a5043"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.835632Z",
     "start_time": "2025-12-27T11:34:14.815973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2idx = {w: idx for idx, w in enumerate(words)}\n",
    "print(f\"Words to Index:  {word2idx}\")\n",
    "\n",
    "idx2word = {idx: w for w, idx in word2idx.items()}\n",
    "print(f\"Index to Words:  {idx2word}\")"
   ],
   "id": "fc89f08bb3def977",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to Index:  {'samosas': 0, 'i': 1, 'lights': 2, 'friends': 3, '<END>': 4, 'my': 5, 'you': 6, 'Sushovan': 7, 'of': 8, 'train': 9, 'and': 10, 'drinking': 11, 'eating': 12, 'holi': 13, 'love': 14, 'sweets': 15, 'very': 16, 'roads': 17, 'busy': 18, 'it': 19, 'again': 20, 'name': 21, 'raining': 22, 'festival': 23, 'brings': 24, 'hot': 25, 'late': 26, 'hello': 27, 'tea': 28, 'the': 29, 'are': 30, 'cricket': 31, 'Mumbai': 32, 'diwali': 33, 'won': 34, 'match': 35, 'is': 36, 'in': 37, 'favorite': 38, 'india': 39, 'Kolkata': 40, 'how': 41}\n",
      "Index to Words:  {0: 'samosas', 1: 'i', 2: 'lights', 3: 'friends', 4: '<END>', 5: 'my', 6: 'you', 7: 'Sushovan', 8: 'of', 9: 'train', 10: 'and', 11: 'drinking', 12: 'eating', 13: 'holi', 14: 'love', 15: 'sweets', 16: 'very', 17: 'roads', 18: 'busy', 19: 'it', 20: 'again', 21: 'name', 22: 'raining', 23: 'festival', 24: 'brings', 25: 'hot', 26: 'late', 27: 'hello', 28: 'tea', 29: 'the', 30: 'are', 31: 'cricket', 32: 'Mumbai', 33: 'diwali', 34: 'won', 35: 'match', 36: 'is', 37: 'in', 38: 'favorite', 39: 'india', 40: 'Kolkata', 41: 'how'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convert to tensor\n",
    "Replace each word in text with the corresponding index and fed that to a tensor"
   ],
   "id": "c1a24bd5e8ab22fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.879610Z",
     "start_time": "2025-12-27T11:34:14.837022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor([word2idx[idx] for idx in text.split()], dtype=torch.long)\n",
    "print(f\"Tensor Data: {data}\")\n",
    "print(f\"Data Shape: {data.shape}\")"
   ],
   "id": "8ce9d73bf21ca37d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Data: tensor([27,  3, 41, 30,  6,  4, 29, 28, 36, 16, 25,  4,  5, 21, 36,  7,  4, 29,\n",
      "        17,  8, 40, 30, 18,  4, 19, 36, 22, 37, 32,  4, 29,  9, 36, 26, 20,  4,\n",
      "         1, 14, 12,  0, 10, 11, 28,  4, 13, 36,  5, 38, 23,  4, 33, 24,  2, 10,\n",
      "        15,  4, 39, 34, 29, 31, 35,  4], device='mps:0')\n",
      "Data Shape: torch.Size([62])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Parameters",
   "id": "49766130e430a24e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`block_size`: also known as context_length. It means that how many previous words the llm would refer, to predict the next word",
   "id": "2028cc337be30be2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.891625Z",
     "start_time": "2025-12-27T11:34:14.881341Z"
    }
   },
   "cell_type": "code",
   "source": "block_size = 6",
   "id": "e74b16de9dc1fb86",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`embedding_dim`: Embedding Dimension\n",
    "\n",
    "Each word in the tensor will be represented in the llm model as a dimensional vector of a defined size. Initially, random numbers are generate for each value in the dimensional vector. The llm uses this vector to predict the next word."
   ],
   "id": "2e859c493861ac7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.923600Z",
     "start_time": "2025-12-27T11:34:14.892473Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_dim = 32",
   "id": "ff3d173e49ccd307",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.937947Z",
     "start_time": "2025-12-27T11:34:14.931799Z"
    }
   },
   "cell_type": "code",
   "source": "n_heads = 2  # number of Multi-head attention layer",
   "id": "cd196a5527526411",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.953286Z",
     "start_time": "2025-12-27T11:34:14.943746Z"
    }
   },
   "cell_type": "code",
   "source": "n_layers = 2  # number of transformer blocks to use",
   "id": "bba67c5b4611a456",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.958265Z",
     "start_time": "2025-12-27T11:34:14.954053Z"
    }
   },
   "cell_type": "code",
   "source": "lr = 1e-3  # learning rate",
   "id": "131917c8e2d869eb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:14.962451Z",
     "start_time": "2025-12-27T11:34:14.958836Z"
    }
   },
   "cell_type": "code",
   "source": "epochs = 1500  # number of training iterations",
   "id": "5726df92e353910b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define batch function",
   "id": "b49108b7ca8dc59c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`batch_size`: This indicates the number of sequences (or sentences) for the model to consider",
   "id": "2dd015dabd7b4066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:15.167290Z",
     "start_time": "2025-12-27T11:34:14.968977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(batch_size=16):\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    # the above function gives 16 examples of random sentences\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return ix, x, y\n",
    "\n",
    "\n",
    "rand_int, input_batch, output_bath = get_batch()\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Batch Index: {rand_int}\")\n",
    "print(f\"input_batch shape: {input_batch.data}\")\n",
    "print(f\"output_bath shape: {output_bath.data}\")"
   ],
   "id": "684d39b37702b8ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: tensor([27,  3, 41, 30,  6,  4, 29, 28, 36, 16, 25,  4,  5, 21, 36,  7,  4, 29,\n",
      "        17,  8, 40, 30, 18,  4, 19, 36, 22, 37, 32,  4, 29,  9, 36, 26, 20,  4,\n",
      "         1, 14, 12,  0, 10, 11, 28,  4, 13, 36,  5, 38, 23,  4, 33, 24,  2, 10,\n",
      "        15,  4, 39, 34, 29, 31, 35,  4], device='mps:0')\n",
      "Batch Index: tensor([38, 49, 52,  2, 20, 29,  4, 26, 29, 47, 35,  6, 28, 13, 38, 47],\n",
      "       device='mps:0')\n",
      "input_batch shape: tensor([[12,  0, 10, 11, 28,  4],\n",
      "        [ 4, 33, 24,  2, 10, 15],\n",
      "        [ 2, 10, 15,  4, 39, 34],\n",
      "        [41, 30,  6,  4, 29, 28],\n",
      "        [40, 30, 18,  4, 19, 36],\n",
      "        [ 4, 29,  9, 36, 26, 20],\n",
      "        [ 6,  4, 29, 28, 36, 16],\n",
      "        [22, 37, 32,  4, 29,  9],\n",
      "        [ 4, 29,  9, 36, 26, 20],\n",
      "        [38, 23,  4, 33, 24,  2],\n",
      "        [ 4,  1, 14, 12,  0, 10],\n",
      "        [29, 28, 36, 16, 25,  4],\n",
      "        [32,  4, 29,  9, 36, 26],\n",
      "        [21, 36,  7,  4, 29, 17],\n",
      "        [12,  0, 10, 11, 28,  4],\n",
      "        [38, 23,  4, 33, 24,  2]], device='mps:0')\n",
      "output_bath shape: tensor([[ 0, 10, 11, 28,  4, 13],\n",
      "        [33, 24,  2, 10, 15,  4],\n",
      "        [10, 15,  4, 39, 34, 29],\n",
      "        [30,  6,  4, 29, 28, 36],\n",
      "        [30, 18,  4, 19, 36, 22],\n",
      "        [29,  9, 36, 26, 20,  4],\n",
      "        [ 4, 29, 28, 36, 16, 25],\n",
      "        [37, 32,  4, 29,  9, 36],\n",
      "        [29,  9, 36, 26, 20,  4],\n",
      "        [23,  4, 33, 24,  2, 10],\n",
      "        [ 1, 14, 12,  0, 10, 11],\n",
      "        [28, 36, 16, 25,  4,  5],\n",
      "        [ 4, 29,  9, 36, 26, 20],\n",
      "        [36,  7,  4, 29, 17,  8],\n",
      "        [ 0, 10, 11, 28,  4, 13],\n",
      "        [23,  4, 33, 24,  2, 10]], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:15.240375Z",
     "start_time": "2025-12-27T11:34:15.169831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Decoded data: \\n{[idx2word[i.item()] for i in data]}\")\n",
    "print(f\"input stack words: \\n{np.array([[idx2word[i.item()] for i in b] for b in input_batch])}\")\n",
    "print(f\"output stack words: \\n{np.array([[idx2word[i.item()] for i in b] for b in output_bath])}\")"
   ],
   "id": "16c203bbe77abd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded data: \n",
      "['hello', 'friends', 'how', 'are', 'you', '<END>', 'the', 'tea', 'is', 'very', 'hot', '<END>', 'my', 'name', 'is', 'Sushovan', '<END>', 'the', 'roads', 'of', 'Kolkata', 'are', 'busy', '<END>', 'it', 'is', 'raining', 'in', 'Mumbai', '<END>', 'the', 'train', 'is', 'late', 'again', '<END>', 'i', 'love', 'eating', 'samosas', 'and', 'drinking', 'tea', '<END>', 'holi', 'is', 'my', 'favorite', 'festival', '<END>', 'diwali', 'brings', 'lights', 'and', 'sweets', '<END>', 'india', 'won', 'the', 'cricket', 'match', '<END>']\n",
      "input stack words: \n",
      "[['eating' 'samosas' 'and' 'drinking' 'tea' '<END>']\n",
      " ['<END>' 'diwali' 'brings' 'lights' 'and' 'sweets']\n",
      " ['lights' 'and' 'sweets' '<END>' 'india' 'won']\n",
      " ['how' 'are' 'you' '<END>' 'the' 'tea']\n",
      " ['Kolkata' 'are' 'busy' '<END>' 'it' 'is']\n",
      " ['<END>' 'the' 'train' 'is' 'late' 'again']\n",
      " ['you' '<END>' 'the' 'tea' 'is' 'very']\n",
      " ['raining' 'in' 'Mumbai' '<END>' 'the' 'train']\n",
      " ['<END>' 'the' 'train' 'is' 'late' 'again']\n",
      " ['favorite' 'festival' '<END>' 'diwali' 'brings' 'lights']\n",
      " ['<END>' 'i' 'love' 'eating' 'samosas' 'and']\n",
      " ['the' 'tea' 'is' 'very' 'hot' '<END>']\n",
      " ['Mumbai' '<END>' 'the' 'train' 'is' 'late']\n",
      " ['name' 'is' 'Sushovan' '<END>' 'the' 'roads']\n",
      " ['eating' 'samosas' 'and' 'drinking' 'tea' '<END>']\n",
      " ['favorite' 'festival' '<END>' 'diwali' 'brings' 'lights']]\n",
      "output stack words: \n",
      "[['samosas' 'and' 'drinking' 'tea' '<END>' 'holi']\n",
      " ['diwali' 'brings' 'lights' 'and' 'sweets' '<END>']\n",
      " ['and' 'sweets' '<END>' 'india' 'won' 'the']\n",
      " ['are' 'you' '<END>' 'the' 'tea' 'is']\n",
      " ['are' 'busy' '<END>' 'it' 'is' 'raining']\n",
      " ['the' 'train' 'is' 'late' 'again' '<END>']\n",
      " ['<END>' 'the' 'tea' 'is' 'very' 'hot']\n",
      " ['in' 'Mumbai' '<END>' 'the' 'train' 'is']\n",
      " ['the' 'train' 'is' 'late' 'again' '<END>']\n",
      " ['festival' '<END>' 'diwali' 'brings' 'lights' 'and']\n",
      " ['i' 'love' 'eating' 'samosas' 'and' 'drinking']\n",
      " ['tea' 'is' 'very' 'hot' '<END>' 'my']\n",
      " ['<END>' 'the' 'train' 'is' 'late' 'again']\n",
      " ['is' 'Sushovan' '<END>' 'the' 'roads' 'of']\n",
      " ['samosas' 'and' 'drinking' 'tea' '<END>' 'holi']\n",
      " ['festival' '<END>' 'diwali' 'brings' 'lights' 'and']]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build the model",
   "id": "f91ca4a3fca56216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:34:15.250042Z",
     "start_time": "2025-12-27T11:34:15.241972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformer.transformer_utils import Block\n",
    "\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embeddings = nn.Embedding(block_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_embeddings(idx)\n",
    "        position_embeddings = self.positional_embeddings(torch.arange(T, device=idx.device))\n",
    "\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), targets.view(B * T))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens=10):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, 1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n"
   ],
   "id": "297fd8705f35d6d7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "7229c54e89b5e2e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:35:03.801723Z",
     "start_time": "2025-12-27T11:34:15.250565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TinyGPT()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for step in range(epochs):\n",
    "    _, xb, yb = get_batch()\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {loss.item():.4f}\")\n",
    "    elif step == epochs - 1:\n",
    "        print(f\"Step: {step}, Loss: {loss.item():.4f}\")"
   ],
   "id": "bdc36c7ac8afac31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 3.8710\n",
      "Step: 100, Loss: 1.1697\n",
      "Step: 200, Loss: 0.2769\n",
      "Step: 300, Loss: 0.1886\n",
      "Step: 400, Loss: 0.1711\n",
      "Step: 500, Loss: 0.2098\n",
      "Step: 600, Loss: 0.1316\n",
      "Step: 700, Loss: 0.1889\n",
      "Step: 800, Loss: 0.1220\n",
      "Step: 900, Loss: 0.0810\n",
      "Step: 1000, Loss: 0.0703\n",
      "Step: 1100, Loss: 0.0854\n",
      "Step: 1200, Loss: 0.1881\n",
      "Step: 1300, Loss: 0.0728\n",
      "Step: 1400, Loss: 0.0763\n",
      "Step: 1499, Loss: 0.0673\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Execute the model",
   "id": "8fbe7240922088ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:35:03.808851Z",
     "start_time": "2025-12-27T11:35:03.802447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_model(context):\n",
    "    word_indexed = [word2idx[word] for word in context.split()]\n",
    "    context = torch.tensor([word_indexed], dtype=torch.long)\n",
    "    out = model.generate(context)\n",
    "\n",
    "    print(\"\\nGenerated Text:\\n\")\n",
    "    print(\" \".join(idx2word[int(i)] for i in out[0]))"
   ],
   "id": "f66524b8ee3dc19e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:35:03.949278Z",
     "start_time": "2025-12-27T11:35:03.809363Z"
    }
   },
   "cell_type": "code",
   "source": "execute_model(\"hello\")",
   "id": "62a59adeaff97bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "hello friends how are you <END> the tea is very hot\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:35:03.998573Z",
     "start_time": "2025-12-27T11:35:03.949933Z"
    }
   },
   "cell_type": "code",
   "source": "execute_model(\"my name\")",
   "id": "8967a2cbb48aacdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "my name is Sushovan <END> the roads of Kolkata are busy <END>\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
